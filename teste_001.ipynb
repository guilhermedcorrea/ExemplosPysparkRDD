{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import*\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id\n",
    "from pyspark.sql.functions import col, regexp_replace\n",
    "from pyspark.sql.types import FloatType\n",
    "import os\n",
    "from pyspark.sql.functions import dayofmonth, hour, dayofyear\n",
    "from pyspark.sql.functions import col, to_date, when\n",
    "from pyspark.sql.types import IntegerType,BooleanType,DateType\n",
    "import findspark\n",
    "from pyspark.sql.functions import udf\n",
    "findspark.init(\"C:\\spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SPARK_HOME\"]=r\"C:\\spark\"\n",
    "os.environ[\"JAVA_HOME\"]=r\"C:\\Program Files\\Java\\jdk-20\"\n",
    "os.environ[\"HADOOP_HOME\"]=r\"C:\\spark\\hadoop\\bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\",\"teste\")\n",
    "conf.set(\"spark.master\",\"local[3]\")\n",
    "spark = SparkSession.builder\\\n",
    "    .config(conf=conf)\\\n",
    "    .appName(\"teste\")\\\n",
    "    .master(\"local[3]\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\spark_treinamento\\data_superstore.csv\"\n",
    "\n",
    "\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True,sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Sales: float, Profit: float]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "columns_to_process = [\"Sales\", \"Profit\"]\n",
    "\n",
    "df = df.select(*[\n",
    "    udf(\n",
    "        lambda value, col_name=col_name: float(value.replace(\",\", \".\")) if value else None,\n",
    "        FloatType()\n",
    "    )(col(col_name)).alias(col_name)\n",
    "    for col_name in columns_to_process\n",
    "])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+----------+----------+--------------+----------+------------------+-----------+-------------+---------------+--------------+----------+-------+---------------+---------------+------------+--------------------+--------+--------+--------+--------+\n",
      "|RowID|       OrderID| OrderDate|  ShipDate|      ShipMode|CustomerID|      CustomerName|    Segment|      Country|           City|         State|PostalCode| Region|      ProductID|       Category|Sub-Category|         ProductName|   Sales|Quantity|Discount|  Profit|\n",
      "+-----+--------------+----------+----------+--------------+----------+------------------+-----------+-------------+---------------+--------------+----------+-------+---------------+---------------+------------+--------------------+--------+--------+--------+--------+\n",
      "|    1|CA-2016-152156|08/11/2016|11/11/2016|  Second Class|  CG-12520|       Claire Gute|   Consumer|United States|      Henderson|      Kentucky|     42420|  South|FUR-BO-10001798|      Furniture|   Bookcases|Bush Somerset Col...|  261.96|       2|       0| 41.9136|\n",
      "|    2|CA-2016-152156|08/11/2016|11/11/2016|  Second Class|  CG-12520|       Claire Gute|   Consumer|United States|      Henderson|      Kentucky|     42420|  South|FUR-CH-10000454|      Furniture|      Chairs|Hon Deluxe Fabric...|  731.94|       3|       0| 219.582|\n",
      "|    3|CA-2016-138688|12/06/2016|16/06/2016|  Second Class|  DV-13045|   Darrin Van Huff|  Corporate|United States|    Los Angeles|    California|     90036|   West|OFF-LA-10000240|Office Supplies|      Labels|Self-Adhesive Add...|   14.62|       2|       0|  6.8714|\n",
      "|    4|US-2015-108966|11/10/2015|18/10/2015|Standard Class|  SO-20335|    Sean O'Donnell|   Consumer|United States|Fort Lauderdale|       Florida|     33311|  South|FUR-TA-10000577|      Furniture|      Tables|Bretford CR4500 S...|957.5775|       5|    0,45|-383.031|\n",
      "|    5|US-2015-108966|11/10/2015|18/10/2015|Standard Class|  SO-20335|    Sean O'Donnell|   Consumer|United States|Fort Lauderdale|       Florida|     33311|  South|OFF-ST-10000760|Office Supplies|     Storage|Eldon Fold 'N Rol...|  22.368|       2|     0,2|  2.5164|\n",
      "|    6|CA-2014-115812|09/06/2014|14/06/2014|Standard Class|  BH-11710|   Brosina Hoffman|   Consumer|United States|    Los Angeles|    California|     90032|   West|FUR-FU-10001487|      Furniture| Furnishings|Eldon Expressions...|   48.86|       7|       0| 14.1694|\n",
      "|    7|CA-2014-115812|09/06/2014|14/06/2014|Standard Class|  BH-11710|   Brosina Hoffman|   Consumer|United States|    Los Angeles|    California|     90032|   West|OFF-AR-10002833|Office Supplies|         Art|          Newell 322|    7.28|       4|       0|  1.9656|\n",
      "|    8|CA-2014-115812|09/06/2014|14/06/2014|Standard Class|  BH-11710|   Brosina Hoffman|   Consumer|United States|    Los Angeles|    California|     90032|   West|TEC-PH-10002275|     Technology|      Phones|Mitel 5320 IP Pho...| 907.152|       6|     0,2| 90.7152|\n",
      "|    9|CA-2014-115812|09/06/2014|14/06/2014|Standard Class|  BH-11710|   Brosina Hoffman|   Consumer|United States|    Los Angeles|    California|     90032|   West|OFF-BI-10003910|Office Supplies|     Binders|DXL Angle-View Bi...|  18.504|       3|     0,2|  5.7825|\n",
      "|   10|CA-2014-115812|09/06/2014|14/06/2014|Standard Class|  BH-11710|   Brosina Hoffman|   Consumer|United States|    Los Angeles|    California|     90032|   West|OFF-AP-10002892|Office Supplies|  Appliances|Belkin F5C206VTEL...|   114.9|       5|       0|   34.47|\n",
      "|   11|CA-2014-115812|09/06/2014|14/06/2014|Standard Class|  BH-11710|   Brosina Hoffman|   Consumer|United States|    Los Angeles|    California|     90032|   West|FUR-TA-10001539|      Furniture|      Tables|Chromcraft Rectan...|1706.184|       9|     0,2| 85.3092|\n",
      "|   12|CA-2014-115812|09/06/2014|14/06/2014|Standard Class|  BH-11710|   Brosina Hoffman|   Consumer|United States|    Los Angeles|    California|     90032|   West|TEC-PH-10002033|     Technology|      Phones|Konftel 250 Confe...| 911.424|       4|     0,2| 68.3568|\n",
      "|   13|CA-2017-114412|15/04/2017|20/04/2017|Standard Class|  AA-10480|      Andrew Allen|   Consumer|United States|        Concord|North Carolina|     28027|  South|OFF-PA-10002365|Office Supplies|       Paper|          Xerox 1967|  15.552|       3|     0,2|  5.4432|\n",
      "|   14|CA-2016-161389|05/12/2016|10/12/2016|Standard Class|  IM-15070|      Irene Maddox|   Consumer|United States|        Seattle|    Washington|     98103|   West|OFF-BI-10003656|Office Supplies|     Binders|Fellowes PB200 Pl...| 407.976|       3|     0,2|132.5922|\n",
      "|   15|US-2015-118983|22/11/2015|26/11/2015|Standard Class|  HP-14815|     Harold Pawlan|Home Office|United States|     Fort Worth|         Texas|     76106|Central|OFF-AP-10002311|Office Supplies|  Appliances|Holmes Replacemen...|   68.81|       5|     0,8|-123.858|\n",
      "|   16|US-2015-118983|22/11/2015|26/11/2015|Standard Class|  HP-14815|     Harold Pawlan|Home Office|United States|     Fort Worth|         Texas|     76106|Central|OFF-BI-10000756|Office Supplies|     Binders|Storex DuraTech R...|   2.544|       3|     0,8|  -3.816|\n",
      "|   17|CA-2014-105893|11/11/2014|18/11/2014|Standard Class|  PK-19075|         Pete Kriz|   Consumer|United States|        Madison|     Wisconsin|     53711|Central|OFF-ST-10004186|Office Supplies|     Storage|\"Stur-D-Stor Shel...|  665.88|       6|       0| 13.3176|\n",
      "|   18|CA-2014-167164|13/05/2014|15/05/2014|  Second Class|  AG-10270|   Alejandro Grove|   Consumer|United States|    West Jordan|          Utah|     84084|   West|OFF-ST-10000107|Office Supplies|     Storage|Fellowes Super St...|    55.5|       2|       0|    9.99|\n",
      "|   19|CA-2014-143336|27/08/2014|01/09/2014|  Second Class|  ZD-21925|Zuschuss Donatelli|   Consumer|United States|  San Francisco|    California|     94109|   West|OFF-AR-10003056|Office Supplies|         Art|          Newell 341|    8.56|       2|       0|  2.4824|\n",
      "|   20|CA-2014-143336|27/08/2014|01/09/2014|  Second Class|  ZD-21925|Zuschuss Donatelli|   Consumer|United States|  San Francisco|    California|     94109|   West|TEC-PH-10001949|     Technology|      Phones|Cisco SPA 501G IP...|  213.48|       3|     0,2|  16.011|\n",
      "+-----+--------------+----------+----------+--------------+----------+------------------+-----------+-------------+---------------+--------------+----------+-------+---------------+---------------+------------+--------------------+--------+--------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import FloatType\n",
    "import os\n",
    "import findspark\n",
    "\n",
    "findspark.init(r\"C:\\spark\")\n",
    "os.environ[\"SPARK_HOME\"] = r\"C:\\spark\"\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\spark\\hadoop\\bin\"\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Java\\jdk-20\"\n",
    "\n",
    "spark = SparkSession.builder.appName(\"read_csv\").getOrCreate()\n",
    "\n",
    "file_path = r\"C:\\spark_treinamento\\data_superstore.csv\"\n",
    "\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True, sep=\";\")\n",
    "\n",
    "\n",
    "columns_to_process = [\"Sales\", \"Profit\"]\n",
    "\n",
    "\n",
    "df_corrected = df.select(\n",
    "    *[col(col_name).cast(FloatType()).alias(col_name) if col_name in columns_to_process else col(col_name)\n",
    "      for col_name in df.columns]\n",
    ")\n",
    "\n",
    "\n",
    "df_corrected = df.withColumn(\"Sales\", regexp_replace(col(\"Sales\"), \",\", \".\").cast(FloatType()))\n",
    "df_corrected = df_corrected.withColumn(\"Profit\", regexp_replace(col(\"Profit\"), \",\", \".\").cast(FloatType()))\n",
    "\n",
    "\n",
    "df_corrected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+----------+----------+------------+----------+---------------+---------+-------------+-----------+----------+----------+------+---------------+---------------+------------+--------------------+------+--------+--------+-------+\n",
      "|RowID|       OrderID| OrderDate|  ShipDate|    ShipMode|CustomerID|   CustomerName|  Segment|      Country|       City|     State|PostalCode|Region|      ProductID|       Category|Sub-Category|         ProductName| Sales|Quantity|Discount| Profit|\n",
      "+-----+--------------+----------+----------+------------+----------+---------------+---------+-------------+-----------+----------+----------+------+---------------+---------------+------------+--------------------+------+--------+--------+-------+\n",
      "|    1|CA-2016-152156|08/11/2016|11/11/2016|Second Class|  CG-12520|    Claire Gute| Consumer|United States|  Henderson|  Kentucky|     42420| South|FUR-BO-10001798|      Furniture|   Bookcases|Bush Somerset Col...|261.96|       2|       0|41.9136|\n",
      "|    2|CA-2016-152156|08/11/2016|11/11/2016|Second Class|  CG-12520|    Claire Gute| Consumer|United States|  Henderson|  Kentucky|     42420| South|FUR-CH-10000454|      Furniture|      Chairs|Hon Deluxe Fabric...|731.94|       3|       0|219.582|\n",
      "|    3|CA-2016-138688|12/06/2016|16/06/2016|Second Class|  DV-13045|Darrin Van Huff|Corporate|United States|Los Angeles|California|     90036|  West|OFF-LA-10000240|Office Supplies|      Labels|Self-Adhesive Add...| 14.62|       2|       0| 6.8714|\n",
      "+-----+--------------+----------+----------+------------+----------+---------------+---------+-------------+-----------+----------+----------+------+---------------+---------------+------------+--------------------+------+--------+--------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace\n",
    "from pyspark.sql.types import FloatType\n",
    "import os\n",
    "import findspark\n",
    "\n",
    "findspark.init(r\"C:\\spark\")\n",
    "os.environ[\"SPARK_HOME\"] = r\"C:\\spark\"\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\spark\\hadoop\\bin\"\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Java\\jdk-20\"\n",
    "\n",
    "spark = SparkSession.builder.appName(\"read_csv\").getOrCreate()\n",
    "\n",
    "file_path = r\"C:\\spark_treinamento\\data_superstore.csv\"\n",
    "\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True, sep=\";\")\n",
    "\n",
    "\n",
    "columns_to_process = [\"Sales\", \"Profit\"]\n",
    "\n",
    "\n",
    "df_corrected = df.select(\n",
    "    *[regexp_replace(col(col_name), \",\", \".\").cast(FloatType()).alias(col_name)\n",
    "      if col_name in columns_to_process else col(col_name)\n",
    "      for col_name in df.columns]\n",
    ")\n",
    "\n",
    "\n",
    "df_corrected.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace\n",
    "from pyspark.sql.types import FloatType\n",
    "import os\n",
    "import findspark\n",
    "\n",
    "findspark.init(r\"C:\\spark\")\n",
    "os.environ[\"SPARK_HOME\"] = r\"C:\\spark\"\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\spark\\hadoop\\bin\"\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Java\\jdk-20\"\n",
    "\n",
    "spark = SparkSession.builder.appName(\"read_csv\").getOrCreate()\n",
    "\n",
    "file_path = r\"C:\\spark_teste\\data_superstore.csv\"\n",
    "\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True, sep=\";\")\n",
    "\n",
    "\n",
    "df_corrected = df.withColumn(\"Sales\", regexp_replace(col(\"Sales\"), \",\", \".\").cast(FloatType()))\n",
    "df_corrected = df_corrected.withColumn(\"Profit\", regexp_replace(col(\"Profit\"), \",\", \".\").cast(FloatType()))\n",
    "\n",
    "\n",
    "df_corrected.select(\"Sales\", \"Profit\").show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Exemplo\").getOrCreate()\n",
    "\n",
    "\n",
    "def process_column(column):\n",
    "    return udf(\n",
    "        lambda value: float(value.replace(\",\", \".\")) if value else None,\n",
    "        FloatType()\n",
    "    )(column)\n",
    "\n",
    "\n",
    "columns_to_process = [\"Sales\", \"Values\", \"Quantity\"]\n",
    "\n",
    "\n",
    "for col_name in columns_to_process:\n",
    "    df = df.withColumn(col_name, process_column(df[col_name]))\n",
    "\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Exemplo\").getOrCreate()\n",
    "\n",
    "columns_to_process = [\"Sales\", \"Values\", \"Quantity\"]\n",
    "\n",
    "df = df.select(*[\n",
    "    udf(\n",
    "        lambda value, col_name=col_name: float(value.replace(\",\", \".\")) if value else None,\n",
    "        FloatType()\n",
    "    )(col(col_name)).alias(col_name)\n",
    "    for col_name in columns_to_process\n",
    "])\n",
    "\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ano: 2024, Mês: Janeiro, Produto: produto1, Vendas: 2212\n",
      "Ano: 2024, Mês: Fevereiro, Produto: produto1, Vendas: 2212\n",
      "Ano: 2024, Mês: Março, Produto: produto1, Vendas: 2212\n",
      "Ano: 2023, Mês: Janeiro, Produto: produto2, Vendas: 1601\n",
      "Ano: 2023, Mês: Fevereiro, Produto: produto2, Vendas: 1601\n",
      "Ano: 2023, Mês: Março, Produto: produto2, Vendas: 1601\n",
      "Ano: 2023, Mês: Janeiro, Produto: produto1, Vendas: 1530\n",
      "Ano: 2023, Mês: Fevereiro, Produto: produto1, Vendas: 1530\n",
      "Ano: 2023, Mês: Março, Produto: produto1, Vendas: 1530\n",
      "Ano: 2024, Mês: Janeiro, Produto: produto2, Vendas: 1108\n",
      "Ano: 2024, Mês: Fevereiro, Produto: produto2, Vendas: 1108\n",
      "Ano: 2024, Mês: Março, Produto: produto2, Vendas: 1108\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "\n",
    "registros_vendas = {\n",
    "    \"2023\": {\n",
    "        \"Janeiro\": {\"produto1\": randint(1, 1000), \"produto2\": randint(1, 1000)},\n",
    "        \"Fevereiro\": {\"produto1\": randint(1, 1000), \"produto2\": randint(1, 1000)},\n",
    "        \"Março\": {\"produto1\": randint(1, 1000), \"produto2\": randint(1, 1000)},\n",
    "    },\n",
    "    \"2024\": {\n",
    "        \"Janeiro\": {\"produto1\": randint(1, 1000), \"produto2\": randint(1, 1000)},\n",
    "        \"Fevereiro\": {\"produto1\": randint(1, 1000), \"produto2\": randint(1, 1000)},\n",
    "        \"Março\": {\"produto1\": randint(1, 1000), \"produto2\": randint(1, 1000)},\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "vendas_por_produto = {\n",
    "    ano: {\n",
    "        mes: {\n",
    "            produto: sum(registros_vendas[ano][mes][produto] for mes in registros_vendas[ano])\n",
    "            for produto in registros_vendas[ano][mes]\n",
    "        }\n",
    "        for mes in registros_vendas[ano]\n",
    "    }\n",
    "    for ano in registros_vendas\n",
    "}\n",
    "\n",
    "\n",
    "produtos_ordenados = sorted(\n",
    "    ((ano, mes, produto, vendas)\n",
    "    for ano, meses in vendas_por_produto.items()\n",
    "    for mes, produtos in meses.items()\n",
    "    for produto, vendas in produtos.items()),\n",
    "    key=lambda x: x[3],\n",
    "    reverse=True,\n",
    ")\n",
    "\n",
    "\n",
    "indice = 0\n",
    "while indice < len(produtos_ordenados):\n",
    "    ano, mes, produto, vendas = produtos_ordenados[indice]\n",
    "    print(f\"Ano: {ano}, Mês: {mes}, Produto: {produto}, Vendas: {vendas}\")\n",
    "    indice += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Exemplo\").getOrCreate()\n",
    "\n",
    "\n",
    "process_sales = udf(\n",
    "    lambda sales: float(sales.replace(\",\", \".\")) if sales else None,\n",
    "    FloatType()\n",
    ")\n",
    "\n",
    "\n",
    "df = df.withColumn(\"Sales\", process_sales(df[\"Sales\"]))\n",
    "\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1, 'a'), (2, 'b'), (1, 'c')])\n",
    "grouped = rdd.groupByKey()\n",
    "for key, values in grouped.collect():\n",
    "    print(key, list(values))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lista = [1, 2, 3, 4, 5]\n",
    "nova_lista = [x * 2 for x in lista]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "novo_rdd = rdd.map(lambda x: x * 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Exemplo\").getOrCreate()\n",
    "df = spark.createDataFrame([(1,), (2,), (3,), (4,), (5,)], [\"valor\"])\n",
    "novo_df = df.select((df.valor * 2).alias(\"novo_valor\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lista = [1, 2, 3, 4, 5]\n",
    "nova_lista = [x for x in lista if x % 2 == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "novo_rdd = rdd.filter(lambda x: x % 2 == 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Exemplo\").getOrCreate()\n",
    "df = spark.createDataFrame([(1,), (2,), (3,), (4,), (5,)], [\"valor\"])\n",
    "novo_df = df.filter(df.valor % 2 == 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lista = [1, 2, 3, 4, 5]\n",
    "media = sum(lista) / len(lista)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "media = rdd.aggregate((0, 0), lambda acc, x: (acc[0] + x, acc[1] + 1), lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))\n",
    "resultado = media[0] / media[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Exemplo\").getOrCreate()\n",
    "df = spark.createDataFrame([(1,), (2,), (3,), (4,), (5,)], [\"valor\"])\n",
    "resultado = df.agg(avg(\"valor\")).collect()[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lista = [1, 2, 3, 4, 5]\n",
    "resultado = sum([x * 2 for x in lista if x % 2 == 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "resultado = rdd.filter(lambda x: x % 2 == 0).map(lambda x: x * 2).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "fibonacci = [0, 1] + [fibonacci[i - 1] + fibonacci[i - 2] for i in range(2, n)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomes = [\"Alice\", \"Bob\", \"Charlie\", \"David\"]\n",
    "iniciais = [nome[0] for nome in nomes if len(nome) > 4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "diagonal = [linha[i] for i, linha in enumerate(matriz)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = [(\"Alice\", 28), (\"Bob\", 35), (\"Charlie\", 42), (\"David\", 25)]\n",
    "nomes_acima_30 = [nome for nome, idade in dados if (lambda x: x > 30)(idade)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local\", \"Exemplo\")\n",
    "\n",
    "dados = [(\"Nova York\", 25), (\"Los Angeles\", 30), (\"Nova York\", 28), (\"Los Angeles\", 32)]\n",
    "rdd = sc.parallelize(dados)\n",
    "\n",
    "resultado = (\n",
    "    rdd\n",
    "    .map(lambda x: (x[0], (x[1], 1))) \n",
    "    .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])) \n",
    "    .map(lambda x: (x[0], x[1][0] / x[1][1]))  \n",
    "    .sortBy(lambda x: x[1])  \n",
    "    .collect()\n",
    ")\n",
    "\n",
    "print(resultado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Exemplo\").getOrCreate()\n",
    "df = spark.createDataFrame([(1,), (2,), (3,), (4,), (5,)], [\"valor\"])\n",
    "resultado = df.filter(df.valor % 2 == 0).select((df.valor * 2).alias(\"novo_valor\")).agg(sum(\"novo_valor\")).collect()[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Exemplo\").getOrCreate()\n",
    "\n",
    "dados = [(\"Nova York\", 25), (\"Los Angeles\", 30), (\"Nova York\", 28), (\"Los Angeles\", 32)]\n",
    "df = spark.createDataFrame(dados, [\"cidade\", \"temperatura\"])\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "resultado = (\n",
    "    df\n",
    "    .groupBy(\"cidade\")\n",
    "    .agg(F.avg(\"temperatura\").alias(\"media\"))\n",
    "    .orderBy(\"media\")\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Eletrônicos', 3000), ('Roupas', 800)]\n"
     ]
    }
   ],
   "source": [
    "vendas = [\n",
    "    {\"produto\": \"A\", \"categoria\": \"Eletrônicos\", \"valor\": 1000},\n",
    "    {\"produto\": \"B\", \"categoria\": \"Roupas\", \"valor\": 500},\n",
    "    {\"produto\": \"C\", \"categoria\": \"Eletrônicos\", \"valor\": 800},\n",
    "    {\"produto\": \"D\", \"categoria\": \"Roupas\", \"valor\": 300},\n",
    "    {\"produto\": \"E\", \"categoria\": \"Eletrônicos\", \"valor\": 1200},\n",
    "]\n",
    "\n",
    "\n",
    "resultado = sorted(\n",
    "    [(categoria, sum(venda[\"valor\"] for venda in vendas if venda[\"categoria\"] == categoria))\n",
    "     for categoria in set(venda[\"categoria\"] for venda in vendas)],\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "print(resultado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cliente1': {'01': 20.0, '02': 40.0, '03': 60.0}, 'Cliente2': {'01': 30.0, '02': 24.0}}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "pedidos = [\n",
    "    {\"cliente\": \"Cliente1\", \"data\": \"2023-01-15\", \"valor\": 100},\n",
    "    {\"cliente\": \"Cliente2\", \"data\": \"2023-01-20\", \"valor\": 150},\n",
    "    {\"cliente\": \"Cliente1\", \"data\": \"2023-02-10\", \"valor\": 200},\n",
    "    {\"cliente\": \"Cliente2\", \"data\": \"2023-02-18\", \"valor\": 120},\n",
    "    {\"cliente\": \"Cliente1\", \"data\": \"2023-03-05\", \"valor\": 300},\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "def extrair_mes(data):\n",
    "    return data.split(\"-\")[1]\n",
    "\n",
    "\n",
    "gastos_por_cliente_e_mes = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "\"\"\"Media gastos mes\"\"\"\n",
    "\n",
    "for pedido in pedidos:\n",
    "    cliente = pedido[\"cliente\"]\n",
    "    mes = extrair_mes(pedido[\"data\"])\n",
    "    valor = pedido[\"valor\"]\n",
    "    gastos_por_cliente_e_mes[cliente][mes] += valor\n",
    "\n",
    "\n",
    "\"\"\"Média gastos mes\"\"\"\n",
    "media_de_gastos = {\n",
    "    cliente: {\n",
    "        mes: total_gastos / len(pedidos)\n",
    "        for mes, total_gastos in meses.items()\n",
    "    }\n",
    "    for cliente, meses in gastos_por_cliente_e_mes.items()\n",
    "}\n",
    "\n",
    "print(media_de_gastos)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
